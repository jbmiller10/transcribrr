target_file: "app/services/transcription_service.py"
test_file: "app/tests/test_transcription_service.py"

# Issues identified in current test file
current_issues:
  - excessive_mocking:
      description: "54-line function stubs ML frameworks at module level"
      impact: "Tests don't validate actual behavior, only mock interactions"
  - invalid_test_data:
      description: "Creates fake WAV data as b'\\0\\0' which is invalid audio"
      impact: "Not testing with realistic data that would expose real issues"
  - combined_scenarios:
      description: "Tests combine unrelated scenarios in single methods"
      impact: "Difficult to identify what specific behavior failed"
  - missing_coverage:
      description: "Never tests actual transcription logic or real audio processing"
      impact: "Critical business logic remains untested"
  - implementation_details:
      description: "Tests verify mock calls instead of outcomes"
      impact: "Tests break with refactoring even when behavior is preserved"

# Fix strategy
fix_approach:
  principles:
    - "Remove module-level mocking entirely"
    - "Use real test audio files or valid synthetic audio data"
    - "Test actual transcription outcomes, not implementation"
    - "Split combined test scenarios into focused tests"
    - "Add comprehensive error and edge case coverage"
    - "Mock only external boundaries (API calls, file I/O when necessary)"

# Detailed fixes by test area

test_structure_improvements:
  - remove_module_stubbing:
      action: "Delete _ensure_stubbed_heavy_modules() function entirely"
      replacement: "Mock only at method boundaries when absolutely necessary"
      
  - use_fixtures:
      action: "Create reusable test fixtures for common scenarios"
      fixtures_needed:
        - valid_audio_file: "Small WAV file with actual audio data"
        - invalid_audio_file: "Corrupted or unsupported format"
        - empty_audio_file: "Valid WAV header but no audio data"
        - large_audio_file: "File exceeding memory limits"

  - separate_test_classes:
      action: "Split monolithic TestTranscriptionService into focused classes"
      new_classes:
        - TestTranscriptionServiceFileValidation: "File existence and format checks"
        - TestTranscriptionServiceLocalTranscription: "Local model transcription"
        - TestTranscriptionServiceAPITranscription: "OpenAI API transcription"
        - TestTranscriptionServiceMPSTranscription: "Apple Silicon MPS path"
        - TestTranscriptionServiceSpeakerDetection: "Speaker diarization"
        - TestModelManager: "Model loading and caching"

# Specific test improvements

file_validation_tests:
  - test_missing_file:
      current: "Tests FileNotFoundError is raised"
      improvement: "Keep but add tests for permission errors, symlinks, directories"
      
  - test_invalid_audio_format:
      current: "Missing"
      add: "Test with non-audio files (text, images) to verify format validation"
      
  - test_corrupted_audio:
      current: "Missing"
      add: "Test with truncated/corrupted audio files"

local_transcription_tests:
  - test_basic_transcription:
      current: "Mocks entire pipeline, returns hardcoded 'hello'"
      fix: |
        - Use a minimal in-memory model or mock only the model loading
        - Test that audio data is properly preprocessed
        - Verify chunk processing for long audio
        - Test language parameter affects output
        
  - test_device_selection:
      current: "Tests mock method calls for MPS/CUDA detection"
      fix: |
        - Test actual device string returned based on availability
        - Use environment variables to simulate different hardware
        - Test fallback behavior when preferred device unavailable
        
  - test_memory_management:
      current: "Missing"
      add: |
        - Test model caching behavior
        - Test memory cleanup after transcription
        - Test handling of out-of-memory scenarios

api_transcription_tests:
  - test_api_success:
      current: "Creates elaborate fake OpenAI client structure"
      fix: |
        - Mock only requests.post or OpenAI client at boundary
        - Test actual request payload construction
        - Verify API response parsing
        
  - test_api_errors:
      current: "Limited to empty response and constructor exception"
      fix: |
        - Test network timeouts
        - Test rate limiting (429 responses)
        - Test invalid API keys (401/403)
        - Test malformed responses
        - Test partial responses
        
  - test_security_validation:
      current: "Basic check for https:// prefix"
      improvement: |
        - Test URL injection attempts
        - Test API key sanitization in logs
        - Test handling of redirect responses

speaker_detection_tests:
  - test_diarization_success:
      current: "Mocks entire pyannote pipeline with fake segments"
      fix: |
        - Test segment alignment with transcript chunks
        - Test handling of overlapping speakers
        - Test empty segments handling
        - Test speaker label formatting
        
  - test_diarization_errors:
      current: "Single test for exception fallback"
      fix: |
        - Test missing HF auth key
        - Test invalid auth key
        - Test network errors during model download
        - Test unsupported audio format for diarization
        
  - test_performance:
      current: "Missing"
      add: |
        - Test diarization doesn't block on long audio
        - Test memory usage with many speakers
        - Test timeout handling

edge_cases_to_add:
  - concurrent_requests:
      description: "Test multiple simultaneous transcription requests"
      scenarios:
        - "Same file transcribed twice simultaneously"
        - "Different files with same model"
        - "Model switching during transcription"
        
  - resource_limits:
      description: "Test behavior at system limits"
      scenarios:
        - "Very long audio files (>1 hour)"
        - "High sample rate audio (192kHz)"
        - "Multi-channel audio"
        - "Low memory conditions"
        
  - error_recovery:
      description: "Test graceful degradation"
      scenarios:
        - "Model download interruption"
        - "Partial model corruption"
        - "GPU memory fragmentation"
        - "Process killed during transcription"

# Anti-patterns to avoid
avoid_these_patterns:
  - mock_framework_internals: "Never mock torch, transformers internals"
  - test_mock_behavior: "Don't test that mocks were called with specific args"
  - hardcoded_results: "Don't return fixed strings like 'hello' from mocks"
  - module_level_stubs: "Don't modify sys.modules in tests"
  - ignore_async: "Don't ignore threading/async behavior in tests"

# Recommended test utilities
test_utilities_needed:
  - audio_generator:
      purpose: "Create valid WAV files with controllable properties"
      features:
        - "Adjustable duration"
        - "Multiple speakers (via frequency modulation)"
        - "Silence periods"
        - "Various sample rates"
        
  - mock_model_factory:
      purpose: "Create lightweight mock models with predictable behavior"
      features:
        - "Returns transcripts based on audio properties"
        - "Simulates processing time"
        - "Can simulate errors"
        
  - assertion_helpers:
      purpose: "Domain-specific assertions"
      examples:
        - "assert_valid_transcript_format()"
        - "assert_timestamps_sequential()"
        - "assert_speakers_labeled()"

# Implementation priorities
priorities:
  high:
    - "Remove module-level stubbing"
    - "Create valid test audio fixtures"
    - "Test actual file validation"
    - "Add error scenario coverage"
    
  medium:
    - "Split into focused test classes"
    - "Add edge case tests"
    - "Improve assertion specificity"
    - "Add performance tests"
    
  low:
    - "Add parameterized tests for similar scenarios"
    - "Create custom assertion helpers"
    - "Add integration test suite"

# Success metrics
success_criteria:
  - code_coverage: ">90% line coverage of transcription_service.py"
  - mock_reduction: "<30% of test code should be mock setup"
  - test_speed: "All unit tests complete in <5 seconds"
  - test_clarity: "Each test method <20 lines"
  - behavior_focus: "100% of tests verify outcomes, not implementation"
  - error_coverage: "Every public method has at least 3 error tests"